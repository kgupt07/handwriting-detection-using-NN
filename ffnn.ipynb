{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMK9T+X5NkeqW/gmp4AC/wb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgupt07/handwriting-detection-using-NN/blob/main/ffnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDC7yOTlOxI7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/sample_data/mnist_test.csv')\n",
        "#read data above\n",
        "data = np.array(data)\n",
        "data.shape\n",
        "#convert data from a dataframe to an array\n",
        "#note: in the csv dataset, the images are intitally stored as flattened rows, hence the final array is 2d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7122YF1bSdAl",
        "outputId": "9d5d3f24-8096-4761-e9a1-d43901f45fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9999, 785)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.shuffle(data)"
      ],
      "metadata": {
        "id": "QScxxayXUwE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_index = int(0.8 * data.shape[0])\n",
        "training_data = data[:split_index]\n",
        "testing_data = data[split_index:]\n",
        "print(training_data.shape)\n",
        "print(testing_data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_02mq09BRIIw",
        "outputId": "45df2e74-9c1c-4bf7-d05d-73c73640b047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7999, 785)\n",
            "(2000, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the data into x: the actual image part and y: the label.\n",
        "data_X = training_data[ : ,1:]\n",
        "data_Y = training_data[ : ,0]\n",
        "print(\"Example X data: \" + str(data_X[0]))\n",
        "print(\"Example Y data: \" + str(data_Y[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H29toMB_U8ti",
        "outputId": "d9fdb2e0-f515-4651-85ca-1761b6fd4712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example X data: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0  68 255 121   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 151\n",
            " 254 250 221  74  33   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0 187 254 254 254 254 241 152  32   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  19 243 242  99 180 237 254 254 239 186  61   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0  23 254 169   0   0  28  96 180\n",
            " 240 254 216  10   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  23 254 169   0   0   0   0   0  28 254 254  22   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0  84 254 169   0   0   0\n",
            "   0   0   5 254 231  15   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0 112 254 143   0   0   0   0   0  30 254 187   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16 104   6   0\n",
            "   0   0   0   0 115 254 135   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0  16 245 254  51   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0  29 254 244   8   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3 149 254 135\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0  32 254 250  24   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 120 254\n",
            " 189   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0  28 237 249  55   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  64\n",
            " 254 236   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0 179 249 105   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "  48 248 222   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0  59 254 155   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0  59 254  71   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0]\n",
            "Example Y data: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lets define prameters!!\n",
        "\n",
        "#defining sigmoid: smoothening activation function to make our function non-linear\n",
        "def sigmoid(x):\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "#defining its derivative:\n",
        "def sigmoid_derivative(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "#defining loss function: mean square error with a factor of 0.5\n",
        "def loss_function(Y, Y_hat):\n",
        "  return 0.5 * np.sum(np.square(Y-Y_hat))/Y.shape[0]\n",
        "  # the 0.5 exists there to a. cleaner math: derivative wont have factor\n",
        "  # and b. it also works as a scaling for the learning rate.\n",
        "\n",
        "#defining derivative of loss function wrt Y_hat\n",
        "def loss_function_derivative(Y, Y_hat):\n",
        "  return (Y_hat-Y)/Y.shape[0]"
      ],
      "metadata": {
        "id": "emg15LXmW9qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets turn the label to 'one-hot-ecoding' basically make the final fully\n",
        "# connected layer of our ffn\n",
        "\n",
        "def convert_to_one_hot(Y):\n",
        "  num_data_points = Y.shape[0]\n",
        "  num_classes = Y.max() + 1\n",
        "  Y_one_hot = np.zeros((num_data_points, num_classes))\n",
        "  Y_one_hot[np.arange(num_data_points), Y] = 1\n",
        "  return Y_one_hot\n",
        "\n",
        "  #arrange basically generates an array from 0 to the num_data_points and\n",
        "  #feeds it through so we can set the relevant a_ij to 1."
      ],
      "metadata": {
        "id": "aT-9eO0JbRZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        #initializing layer sizes\n",
        "        self.input_size = input_size          #784 = 28*28\n",
        "        self.hidden_size = hidden_size        #16\n",
        "        self.output_size = output_size        #10 coz we have 10 classes of nums\n",
        "        #initializing our 2 weight matrices\n",
        "        self.w1 = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.w2 = np.random.randn(self.hidden_size, self.output_size)\n",
        "        #initializing the bias values\n",
        "        self.b1 = np.zeros(self.hidden_size)\n",
        "        self.b2 = np.zeros(self.output_size)\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        self.z1 = np.dot(X, self.w1) + self.b1\n",
        "        self.a1 = sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.w2) + self.b2\n",
        "        self.a2 = sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward_pass(self, X, Y, learning_rate):\n",
        "      loss = loss_function(Y, self.a2)   #loss is a number\n",
        "\n",
        "      dL_da2 = loss_function_derivative(Y, self.a2)  #this is an array\n",
        "      da2_dz2 = sigmoid_derivative(self.z2)\n",
        "      dL_dz2 = dL_da2 * da2_dz2\n",
        "      dz2_dw2 = self.a1\n",
        "\n",
        "      # calcuate the gradient of the loss with respect to w2\n",
        "      dL_dw2 = np.dot(dz2_dw2.T, dL_dz2)\n",
        "      # calculate the gradient of the loss with respect to b2\n",
        "      dL_db2 = np.sum(dL_dz2, axis=0)\n",
        "\n",
        "      dz2_da1 = self.w2\n",
        "      da1_dz1 = sigmoid_derivative(self.z1)\n",
        "      dL_dz1 = np.dot(dL_dz2, dz2_da1.T) * da1_dz1\n",
        "      dz1_dw1 = X\n",
        "\n",
        "      # calculate the gradient of the loss with respect to w1\n",
        "      dL_dw1 = np.dot(dz1_dw1.T, dL_dz1)\n",
        "      # calculate the gradient of the loss with respect to b1\n",
        "      dL_db1 = np.sum(dL_dz1, axis=0)\n",
        "\n",
        "      # update the weights and biases\n",
        "      self.w2 -= learning_rate * dL_dw2\n",
        "      self.b2 -= learning_rate * dL_db2\n",
        "      self.w1 -= learning_rate * dL_dw1\n",
        "      self.b1 -= learning_rate * dL_db1\n",
        "\n",
        "      return loss\n",
        "\n",
        "  #function to return only a subset of the trianing data so we dont use all of it at once\n",
        "    def get_batch(self, X, Y, batch_size = 32):\n",
        "      num_data_points = X.shape[0]\n",
        "      indices = np.random.choice(num_data_points, batch_size)\n",
        "      return X[indices], Y[indices]\n",
        "\n",
        "    def train(self, X, Y, epochs, learning_rate):\n",
        "      num_batches = X.shape[0] // 32\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "        #lr = learning_rate * (0.001 ** (epoch // 10))\n",
        "        for batch in range(num_batches):\n",
        "          X_batch, Y_batch = self.get_batch(X, Y)\n",
        "          self.forward_pass(X_batch)\n",
        "          loss = self.backward_pass(X_batch, Y_batch, learning_rate)\n",
        "        print(\"Epoch: \" + str(epoch) + \" Loss: \" + str(loss))\n",
        "\n",
        "    def predict(self, X):\n",
        "      return self.forward_pass(X)\n",
        "\n",
        "    def accuracy(self, X, Y):\n",
        "      Y_hat = self.predict(X)\n",
        "      Y_hat = np.argmax(Y_hat, axis=1)\n",
        "      return np.mean(Y_hat == Y)"
      ],
      "metadata": {
        "id": "MJwb0KiCdcVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neuralnetwork = NeuralNetwork(784, 16, 10)\n",
        "\n",
        "training_data_Y_one_hot = convert_to_one_hot(data_Y)\n",
        "\n",
        "# Use a subset of the data for testing\n",
        "training_data_X = data_X[:9000]\n",
        "training_data_Y = training_data_Y_one_hot[:9000]\n",
        "testing_data_X = data_X[9000:]\n",
        "testing_data_Y = data_Y[9000:]\n",
        "\n",
        "neuralnetwork.train(training_data_X, training_data_Y, 1000, 0.1)\n",
        "\n",
        "print(neuralnetwork.accuracy(data_X, data_Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk77j5KSClGY",
        "outputId": "9013d1f3-4634-452a-f684-cfaa6518ff32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 0.45798455217006323\n",
            "Epoch: 1 Loss: 0.48514202350084495\n",
            "Epoch: 2 Loss: 0.4814566475639192\n",
            "Epoch: 3 Loss: 0.42234509369158746\n",
            "Epoch: 4 Loss: 0.48000854680850136\n",
            "Epoch: 5 Loss: 0.4342143803047607\n",
            "Epoch: 6 Loss: 0.4453405109482904\n",
            "Epoch: 7 Loss: 0.42775440595186953\n",
            "Epoch: 8 Loss: 0.4328276649342512\n",
            "Epoch: 9 Loss: 0.4346038232815497\n",
            "Epoch: 10 Loss: 0.39485271984932846\n",
            "Epoch: 11 Loss: 0.49326968629224516\n",
            "Epoch: 12 Loss: 0.36115734433777424\n",
            "Epoch: 13 Loss: 0.3697170794124844\n",
            "Epoch: 14 Loss: 0.41724559230745073\n",
            "Epoch: 15 Loss: 0.3695800743937754\n",
            "Epoch: 16 Loss: 0.44378415394791176\n",
            "Epoch: 17 Loss: 0.3771783684218062\n",
            "Epoch: 18 Loss: 0.32532132836046856\n",
            "Epoch: 19 Loss: 0.28999838927837507\n",
            "Epoch: 20 Loss: 0.35535661746302416\n",
            "Epoch: 21 Loss: 0.37541797348793876\n",
            "Epoch: 22 Loss: 0.3708604662574978\n",
            "Epoch: 23 Loss: 0.36888421603809235\n",
            "Epoch: 24 Loss: 0.3248458027911554\n",
            "Epoch: 25 Loss: 0.33260780729194167\n",
            "Epoch: 26 Loss: 0.29562716606468253\n",
            "Epoch: 27 Loss: 0.3430148601528541\n",
            "Epoch: 28 Loss: 0.4450472078071185\n",
            "Epoch: 29 Loss: 0.3387903747297204\n",
            "Epoch: 30 Loss: 0.31539905738822077\n",
            "Epoch: 31 Loss: 0.30715120044566036\n",
            "Epoch: 32 Loss: 0.3605911096435518\n",
            "Epoch: 33 Loss: 0.2775395448837362\n",
            "Epoch: 34 Loss: 0.27550657925060834\n",
            "Epoch: 35 Loss: 0.29212533504333127\n",
            "Epoch: 36 Loss: 0.3189476116546098\n",
            "Epoch: 37 Loss: 0.2875320618528169\n",
            "Epoch: 38 Loss: 0.31799004010088816\n",
            "Epoch: 39 Loss: 0.29392830733226233\n",
            "Epoch: 40 Loss: 0.28680487436595636\n",
            "Epoch: 41 Loss: 0.2701879612483758\n",
            "Epoch: 42 Loss: 0.283276649475685\n",
            "Epoch: 43 Loss: 0.25994223977044745\n",
            "Epoch: 44 Loss: 0.2780298009102835\n",
            "Epoch: 45 Loss: 0.3034330783445248\n",
            "Epoch: 46 Loss: 0.25738626717724467\n",
            "Epoch: 47 Loss: 0.23513863549450842\n",
            "Epoch: 48 Loss: 0.2234483499188583\n",
            "Epoch: 49 Loss: 0.26226545866097933\n",
            "Epoch: 50 Loss: 0.25761821579439853\n",
            "Epoch: 51 Loss: 0.3344780801458604\n",
            "Epoch: 52 Loss: 0.28976198123579755\n",
            "Epoch: 53 Loss: 0.2686742728525575\n",
            "Epoch: 54 Loss: 0.3049265935117404\n",
            "Epoch: 55 Loss: 0.26510830846997774\n",
            "Epoch: 56 Loss: 0.27789651357937456\n",
            "Epoch: 57 Loss: 0.26685180328312486\n",
            "Epoch: 58 Loss: 0.31450929355832213\n",
            "Epoch: 59 Loss: 0.2338175877402622\n",
            "Epoch: 60 Loss: 0.23134441007970227\n",
            "Epoch: 61 Loss: 0.2749534745148806\n",
            "Epoch: 62 Loss: 0.23605169219116945\n",
            "Epoch: 63 Loss: 0.2072759134561321\n",
            "Epoch: 64 Loss: 0.25148740711146067\n",
            "Epoch: 65 Loss: 0.19383976368834838\n",
            "Epoch: 66 Loss: 0.22423094917589634\n",
            "Epoch: 67 Loss: 0.3072294845830003\n",
            "Epoch: 68 Loss: 0.1998017927743812\n",
            "Epoch: 69 Loss: 0.16375704062472107\n",
            "Epoch: 70 Loss: 0.1840990964929709\n",
            "Epoch: 71 Loss: 0.21308869432662841\n",
            "Epoch: 72 Loss: 0.23478466659293418\n",
            "Epoch: 73 Loss: 0.15558128785510095\n",
            "Epoch: 74 Loss: 0.30380484791283024\n",
            "Epoch: 75 Loss: 0.1738980018177751\n",
            "Epoch: 76 Loss: 0.20343608143312608\n",
            "Epoch: 77 Loss: 0.24282100652416055\n",
            "Epoch: 78 Loss: 0.18438155596730715\n",
            "Epoch: 79 Loss: 0.2084750463714159\n",
            "Epoch: 80 Loss: 0.1677335645460165\n",
            "Epoch: 81 Loss: 0.2250595808515673\n",
            "Epoch: 82 Loss: 0.24541262802507724\n",
            "Epoch: 83 Loss: 0.1823061318328857\n",
            "Epoch: 84 Loss: 0.19367128006837342\n",
            "Epoch: 85 Loss: 0.1808464247626736\n",
            "Epoch: 86 Loss: 0.25213517682591474\n",
            "Epoch: 87 Loss: 0.13528961429214353\n",
            "Epoch: 88 Loss: 0.25095523357552646\n",
            "Epoch: 89 Loss: 0.21295864178876608\n",
            "Epoch: 90 Loss: 0.20219167707920127\n",
            "Epoch: 91 Loss: 0.269763531058394\n",
            "Epoch: 92 Loss: 0.23044893422188892\n",
            "Epoch: 93 Loss: 0.21549776886824745\n",
            "Epoch: 94 Loss: 0.16923656954138425\n",
            "Epoch: 95 Loss: 0.20253177347420565\n",
            "Epoch: 96 Loss: 0.14322320688015214\n",
            "Epoch: 97 Loss: 0.20491984959860807\n",
            "Epoch: 98 Loss: 0.22158466426968274\n",
            "Epoch: 99 Loss: 0.1889740061057063\n",
            "Epoch: 100 Loss: 0.18374045299723976\n",
            "Epoch: 101 Loss: 0.20831382597377127\n",
            "Epoch: 102 Loss: 0.1644946720364841\n",
            "Epoch: 103 Loss: 0.25323121778693436\n",
            "Epoch: 104 Loss: 0.20720779714317927\n",
            "Epoch: 105 Loss: 0.17370327985236023\n",
            "Epoch: 106 Loss: 0.21058576512540783\n",
            "Epoch: 107 Loss: 0.18705759206126502\n",
            "Epoch: 108 Loss: 0.2020930494141443\n",
            "Epoch: 109 Loss: 0.27973021151926303\n",
            "Epoch: 110 Loss: 0.20298949782995995\n",
            "Epoch: 111 Loss: 0.12258323412247196\n",
            "Epoch: 112 Loss: 0.19175167919175407\n",
            "Epoch: 113 Loss: 0.1655095582035533\n",
            "Epoch: 114 Loss: 0.15032642593788648\n",
            "Epoch: 115 Loss: 0.14731093712488363\n",
            "Epoch: 116 Loss: 0.19420679211947503\n",
            "Epoch: 117 Loss: 0.18944501402634278\n",
            "Epoch: 118 Loss: 0.19138263840902558\n",
            "Epoch: 119 Loss: 0.1360967032489799\n",
            "Epoch: 120 Loss: 0.21657142838722718\n",
            "Epoch: 121 Loss: 0.18352530009049325\n",
            "Epoch: 122 Loss: 0.16816184677454948\n",
            "Epoch: 123 Loss: 0.19217328015603088\n",
            "Epoch: 124 Loss: 0.20481323541520763\n",
            "Epoch: 125 Loss: 0.1589018859044067\n",
            "Epoch: 126 Loss: 0.18465746338098085\n",
            "Epoch: 127 Loss: 0.2522559539035367\n",
            "Epoch: 128 Loss: 0.2028988910139382\n",
            "Epoch: 129 Loss: 0.19421588509521376\n",
            "Epoch: 130 Loss: 0.13523061133693895\n",
            "Epoch: 131 Loss: 0.12403418085045513\n",
            "Epoch: 132 Loss: 0.18877971947729438\n",
            "Epoch: 133 Loss: 0.10390288654217897\n",
            "Epoch: 134 Loss: 0.1632915895389117\n",
            "Epoch: 135 Loss: 0.19053154936836203\n",
            "Epoch: 136 Loss: 0.12139792291069024\n",
            "Epoch: 137 Loss: 0.16744430654624942\n",
            "Epoch: 138 Loss: 0.20102829952430437\n",
            "Epoch: 139 Loss: 0.19357040187115387\n",
            "Epoch: 140 Loss: 0.10006817314217781\n",
            "Epoch: 141 Loss: 0.17017844852555766\n",
            "Epoch: 142 Loss: 0.10819952427897384\n",
            "Epoch: 143 Loss: 0.14546919892935223\n",
            "Epoch: 144 Loss: 0.13050715780988953\n",
            "Epoch: 145 Loss: 0.05172775309419195\n",
            "Epoch: 146 Loss: 0.16254086282756214\n",
            "Epoch: 147 Loss: 0.23238920006695585\n",
            "Epoch: 148 Loss: 0.19184770747210417\n",
            "Epoch: 149 Loss: 0.23502129694004692\n",
            "Epoch: 150 Loss: 0.1414590774821337\n",
            "Epoch: 151 Loss: 0.21734688113828599\n",
            "Epoch: 152 Loss: 0.19374521075072743\n",
            "Epoch: 153 Loss: 0.15242402735426713\n",
            "Epoch: 154 Loss: 0.17739896101258001\n",
            "Epoch: 155 Loss: 0.18654747802544713\n",
            "Epoch: 156 Loss: 0.2251048573339363\n",
            "Epoch: 157 Loss: 0.2521871688357515\n",
            "Epoch: 158 Loss: 0.1891280361709678\n",
            "Epoch: 159 Loss: 0.15998433133522133\n",
            "Epoch: 160 Loss: 0.1830356109572948\n",
            "Epoch: 161 Loss: 0.19007051715695705\n",
            "Epoch: 162 Loss: 0.2210676772810345\n",
            "Epoch: 163 Loss: 0.20994243431598017\n",
            "Epoch: 164 Loss: 0.11943585919221743\n",
            "Epoch: 165 Loss: 0.2289834313029025\n",
            "Epoch: 166 Loss: 0.16622084132819875\n",
            "Epoch: 167 Loss: 0.21776074265673495\n",
            "Epoch: 168 Loss: 0.15430729326772388\n",
            "Epoch: 169 Loss: 0.24664265220563208\n",
            "Epoch: 170 Loss: 0.16296883337206336\n",
            "Epoch: 171 Loss: 0.13774300207835777\n",
            "Epoch: 172 Loss: 0.19000652716974248\n",
            "Epoch: 173 Loss: 0.11742588029957103\n",
            "Epoch: 174 Loss: 0.16398890461993376\n",
            "Epoch: 175 Loss: 0.10014163743602851\n",
            "Epoch: 176 Loss: 0.10855783732430672\n",
            "Epoch: 177 Loss: 0.13958602412697058\n",
            "Epoch: 178 Loss: 0.11587702409900721\n",
            "Epoch: 179 Loss: 0.16818301934474986\n",
            "Epoch: 180 Loss: 0.15217146899963263\n",
            "Epoch: 181 Loss: 0.15127829513593652\n",
            "Epoch: 182 Loss: 0.14688965097831452\n",
            "Epoch: 183 Loss: 0.11382372722967919\n",
            "Epoch: 184 Loss: 0.1622435370917122\n",
            "Epoch: 185 Loss: 0.11892598290938586\n",
            "Epoch: 186 Loss: 0.1705475229337221\n",
            "Epoch: 187 Loss: 0.09560120580207887\n",
            "Epoch: 188 Loss: 0.13744925446768191\n",
            "Epoch: 189 Loss: 0.14043635924808393\n",
            "Epoch: 190 Loss: 0.1458434061347714\n",
            "Epoch: 191 Loss: 0.07462224232335189\n",
            "Epoch: 192 Loss: 0.13921441665140546\n",
            "Epoch: 193 Loss: 0.11630425395536637\n",
            "Epoch: 194 Loss: 0.1691178750616546\n",
            "Epoch: 195 Loss: 0.15345579112853897\n",
            "Epoch: 196 Loss: 0.141420847671059\n",
            "Epoch: 197 Loss: 0.12309414069187241\n",
            "Epoch: 198 Loss: 0.2121509107372787\n",
            "Epoch: 199 Loss: 0.1562120912209882\n",
            "Epoch: 200 Loss: 0.1598731106722721\n",
            "Epoch: 201 Loss: 0.10070669009784337\n",
            "Epoch: 202 Loss: 0.1336378457857\n",
            "Epoch: 203 Loss: 0.12933695753317664\n",
            "Epoch: 204 Loss: 0.0912435892566936\n",
            "Epoch: 205 Loss: 0.14545260939536053\n",
            "Epoch: 206 Loss: 0.15948971029656778\n",
            "Epoch: 207 Loss: 0.12846812817980727\n",
            "Epoch: 208 Loss: 0.16789311468401233\n",
            "Epoch: 209 Loss: 0.16919682677846215\n",
            "Epoch: 210 Loss: 0.14910405989596143\n",
            "Epoch: 211 Loss: 0.12443659173813533\n",
            "Epoch: 212 Loss: 0.12226651444411749\n",
            "Epoch: 213 Loss: 0.13598029875218298\n",
            "Epoch: 214 Loss: 0.16368828514695313\n",
            "Epoch: 215 Loss: 0.14902327995535625\n",
            "Epoch: 216 Loss: 0.1487058996927457\n",
            "Epoch: 217 Loss: 0.19712130062345648\n",
            "Epoch: 218 Loss: 0.12612901352248201\n",
            "Epoch: 219 Loss: 0.12204411666041956\n",
            "Epoch: 220 Loss: 0.1159207904152059\n",
            "Epoch: 221 Loss: 0.16801459442659641\n",
            "Epoch: 222 Loss: 0.1614199522606679\n",
            "Epoch: 223 Loss: 0.2254218449974601\n",
            "Epoch: 224 Loss: 0.11777723324226083\n",
            "Epoch: 225 Loss: 0.09923199382350587\n",
            "Epoch: 226 Loss: 0.11104407602220054\n",
            "Epoch: 227 Loss: 0.08225802700927055\n",
            "Epoch: 228 Loss: 0.11198701059064506\n",
            "Epoch: 229 Loss: 0.11778758737308835\n",
            "Epoch: 230 Loss: 0.09924537099349248\n",
            "Epoch: 231 Loss: 0.19700827951018202\n",
            "Epoch: 232 Loss: 0.17176128912682323\n",
            "Epoch: 233 Loss: 0.14926993652011733\n",
            "Epoch: 234 Loss: 0.12326573800475898\n",
            "Epoch: 235 Loss: 0.12995802587548452\n",
            "Epoch: 236 Loss: 0.10920968589694416\n",
            "Epoch: 237 Loss: 0.12623641909049113\n",
            "Epoch: 238 Loss: 0.12163362740635314\n",
            "Epoch: 239 Loss: 0.15166367893248886\n",
            "Epoch: 240 Loss: 0.15078487013596925\n",
            "Epoch: 241 Loss: 0.07682408191018661\n",
            "Epoch: 242 Loss: 0.1316383905274429\n",
            "Epoch: 243 Loss: 0.056263488050706634\n",
            "Epoch: 244 Loss: 0.07374407746003345\n",
            "Epoch: 245 Loss: 0.1650425822818415\n",
            "Epoch: 246 Loss: 0.14317229918494823\n",
            "Epoch: 247 Loss: 0.077523936062033\n",
            "Epoch: 248 Loss: 0.15959343955930094\n",
            "Epoch: 249 Loss: 0.1469533101132992\n",
            "Epoch: 250 Loss: 0.206914920213662\n",
            "Epoch: 251 Loss: 0.12810878599382503\n",
            "Epoch: 252 Loss: 0.186349627786442\n",
            "Epoch: 253 Loss: 0.11414898941240986\n",
            "Epoch: 254 Loss: 0.11730250114622336\n",
            "Epoch: 255 Loss: 0.1501601266664298\n",
            "Epoch: 256 Loss: 0.16245351910196942\n",
            "Epoch: 257 Loss: 0.118498218717193\n",
            "Epoch: 258 Loss: 0.16273461280471788\n",
            "Epoch: 259 Loss: 0.10480847664144623\n",
            "Epoch: 260 Loss: 0.08907456789531047\n",
            "Epoch: 261 Loss: 0.18322010058051974\n",
            "Epoch: 262 Loss: 0.14156876664424617\n",
            "Epoch: 263 Loss: 0.1651145739321378\n",
            "Epoch: 264 Loss: 0.15520134230833887\n",
            "Epoch: 265 Loss: 0.17821575328779596\n",
            "Epoch: 266 Loss: 0.13195724742191492\n",
            "Epoch: 267 Loss: 0.08841531221444396\n",
            "Epoch: 268 Loss: 0.15612431438132493\n",
            "Epoch: 269 Loss: 0.19114916758780032\n",
            "Epoch: 270 Loss: 0.13682623274185676\n",
            "Epoch: 271 Loss: 0.18476850807719483\n",
            "Epoch: 272 Loss: 0.13700217656215868\n",
            "Epoch: 273 Loss: 0.1768009224360243\n",
            "Epoch: 274 Loss: 0.1674883846486145\n",
            "Epoch: 275 Loss: 0.19326696320779221\n",
            "Epoch: 276 Loss: 0.14220505003490094\n",
            "Epoch: 277 Loss: 0.21016379058483692\n",
            "Epoch: 278 Loss: 0.15022929795118167\n",
            "Epoch: 279 Loss: 0.1329933535069724\n",
            "Epoch: 280 Loss: 0.13511961297757594\n",
            "Epoch: 281 Loss: 0.14897904012672986\n",
            "Epoch: 282 Loss: 0.14341727404647375\n",
            "Epoch: 283 Loss: 0.15017449285105328\n",
            "Epoch: 284 Loss: 0.1259747113153944\n",
            "Epoch: 285 Loss: 0.05682535668464378\n",
            "Epoch: 286 Loss: 0.15295996928309563\n",
            "Epoch: 287 Loss: 0.10287497338463174\n",
            "Epoch: 288 Loss: 0.08939998230602052\n",
            "Epoch: 289 Loss: 0.1322599180304531\n",
            "Epoch: 290 Loss: 0.07540490459375834\n",
            "Epoch: 291 Loss: 0.1913215658100817\n",
            "Epoch: 292 Loss: 0.13541892106779668\n",
            "Epoch: 293 Loss: 0.14957201914701265\n",
            "Epoch: 294 Loss: 0.13666838131671588\n",
            "Epoch: 295 Loss: 0.12843096842518817\n",
            "Epoch: 296 Loss: 0.1766018927979202\n",
            "Epoch: 297 Loss: 0.17194484607869454\n",
            "Epoch: 298 Loss: 0.08250067147258903\n",
            "Epoch: 299 Loss: 0.0725428860254569\n",
            "Epoch: 300 Loss: 0.10651733892164458\n",
            "Epoch: 301 Loss: 0.1429452132183187\n",
            "Epoch: 302 Loss: 0.10817334021387559\n",
            "Epoch: 303 Loss: 0.12729026728763088\n",
            "Epoch: 304 Loss: 0.11261861522559913\n",
            "Epoch: 305 Loss: 0.18696748483202158\n",
            "Epoch: 306 Loss: 0.0947040769193766\n",
            "Epoch: 307 Loss: 0.11989693733811585\n",
            "Epoch: 308 Loss: 0.17546599463747162\n",
            "Epoch: 309 Loss: 0.13675839199911327\n",
            "Epoch: 310 Loss: 0.14148090642776326\n",
            "Epoch: 311 Loss: 0.07771803430463323\n",
            "Epoch: 312 Loss: 0.09080531930847352\n",
            "Epoch: 313 Loss: 0.11681231596844352\n",
            "Epoch: 314 Loss: 0.18379215529239468\n",
            "Epoch: 315 Loss: 0.1474006037583091\n",
            "Epoch: 316 Loss: 0.16784953996301816\n",
            "Epoch: 317 Loss: 0.12320412111076236\n",
            "Epoch: 318 Loss: 0.11162960939724188\n",
            "Epoch: 319 Loss: 0.08317593021494035\n",
            "Epoch: 320 Loss: 0.1662318100891476\n",
            "Epoch: 321 Loss: 0.11277735161795843\n",
            "Epoch: 322 Loss: 0.10418880562350694\n",
            "Epoch: 323 Loss: 0.12236599344487298\n",
            "Epoch: 324 Loss: 0.14589466828959574\n",
            "Epoch: 325 Loss: 0.16364000210428004\n",
            "Epoch: 326 Loss: 0.08170273200234954\n",
            "Epoch: 327 Loss: 0.17967809712374144\n",
            "Epoch: 328 Loss: 0.16408474740761486\n",
            "Epoch: 329 Loss: 0.10302126938652133\n",
            "Epoch: 330 Loss: 0.13888709527894721\n",
            "Epoch: 331 Loss: 0.08318097667711329\n",
            "Epoch: 332 Loss: 0.1437907348571431\n",
            "Epoch: 333 Loss: 0.13010026001970582\n",
            "Epoch: 334 Loss: 0.1014696612622599\n",
            "Epoch: 335 Loss: 0.09536047166375144\n",
            "Epoch: 336 Loss: 0.09920850739205517\n",
            "Epoch: 337 Loss: 0.07975826884359909\n",
            "Epoch: 338 Loss: 0.07292080414339853\n",
            "Epoch: 339 Loss: 0.10942560144979954\n",
            "Epoch: 340 Loss: 0.12531026297580417\n",
            "Epoch: 341 Loss: 0.08221718595595329\n",
            "Epoch: 342 Loss: 0.09495337143736245\n",
            "Epoch: 343 Loss: 0.14627553600999266\n",
            "Epoch: 344 Loss: 0.16458341179528163\n",
            "Epoch: 345 Loss: 0.10707782463057205\n",
            "Epoch: 346 Loss: 0.15066949894137038\n",
            "Epoch: 347 Loss: 0.13925014433869665\n",
            "Epoch: 348 Loss: 0.06019703199618088\n",
            "Epoch: 349 Loss: 0.0827278470391227\n",
            "Epoch: 350 Loss: 0.15394543036130692\n",
            "Epoch: 351 Loss: 0.16799884594763642\n",
            "Epoch: 352 Loss: 0.1005292425622299\n",
            "Epoch: 353 Loss: 0.160142526048948\n",
            "Epoch: 354 Loss: 0.11438027572608923\n",
            "Epoch: 355 Loss: 0.06284049988457581\n",
            "Epoch: 356 Loss: 0.06981691669435033\n",
            "Epoch: 357 Loss: 0.18345767336127397\n",
            "Epoch: 358 Loss: 0.15435030017206197\n",
            "Epoch: 359 Loss: 0.13272272293283877\n",
            "Epoch: 360 Loss: 0.24541858491573945\n",
            "Epoch: 361 Loss: 0.12340433768954087\n",
            "Epoch: 362 Loss: 0.15941894135081958\n",
            "Epoch: 363 Loss: 0.1815261365415794\n",
            "Epoch: 364 Loss: 0.11229707711465786\n",
            "Epoch: 365 Loss: 0.1297505465140418\n",
            "Epoch: 366 Loss: 0.17254874945418244\n",
            "Epoch: 367 Loss: 0.14691456408587333\n",
            "Epoch: 368 Loss: 0.1423247230617657\n",
            "Epoch: 369 Loss: 0.1551747582567658\n",
            "Epoch: 370 Loss: 0.17467501008112046\n",
            "Epoch: 371 Loss: 0.1334812160211401\n",
            "Epoch: 372 Loss: 0.1451931765512977\n",
            "Epoch: 373 Loss: 0.09395127287608764\n",
            "Epoch: 374 Loss: 0.09974254629892235\n",
            "Epoch: 375 Loss: 0.13115971638318086\n",
            "Epoch: 376 Loss: 0.1728477052251322\n",
            "Epoch: 377 Loss: 0.10780214311216361\n",
            "Epoch: 378 Loss: 0.11280264409075941\n",
            "Epoch: 379 Loss: 0.13394941242877967\n",
            "Epoch: 380 Loss: 0.07793420826966743\n",
            "Epoch: 381 Loss: 0.13327278686305938\n",
            "Epoch: 382 Loss: 0.12466702796868952\n",
            "Epoch: 383 Loss: 0.1452319553753686\n",
            "Epoch: 384 Loss: 0.11077453194698303\n",
            "Epoch: 385 Loss: 0.12384202600986277\n",
            "Epoch: 386 Loss: 0.09540754339241\n",
            "Epoch: 387 Loss: 0.25200015093611183\n",
            "Epoch: 388 Loss: 0.173735495278435\n",
            "Epoch: 389 Loss: 0.1153500689164592\n",
            "Epoch: 390 Loss: 0.07308009583578312\n",
            "Epoch: 391 Loss: 0.11013397302398001\n",
            "Epoch: 392 Loss: 0.0941861909780829\n",
            "Epoch: 393 Loss: 0.13349095138028155\n",
            "Epoch: 394 Loss: 0.15237435262697546\n",
            "Epoch: 395 Loss: 0.15143561357636567\n",
            "Epoch: 396 Loss: 0.13503670281770086\n",
            "Epoch: 397 Loss: 0.1606414902064297\n",
            "Epoch: 398 Loss: 0.039340348825068955\n",
            "Epoch: 399 Loss: 0.13444864503142073\n",
            "Epoch: 400 Loss: 0.08613716222495961\n",
            "Epoch: 401 Loss: 0.08563153899001676\n",
            "Epoch: 402 Loss: 0.15620376601815156\n",
            "Epoch: 403 Loss: 0.10970373654699828\n",
            "Epoch: 404 Loss: 0.06261809464489405\n",
            "Epoch: 405 Loss: 0.11518638828406713\n",
            "Epoch: 406 Loss: 0.15632963351153178\n",
            "Epoch: 407 Loss: 0.14450326741465894\n",
            "Epoch: 408 Loss: 0.14790912706943343\n",
            "Epoch: 409 Loss: 0.09183814229248265\n",
            "Epoch: 410 Loss: 0.13767566357724098\n",
            "Epoch: 411 Loss: 0.15143701117958994\n",
            "Epoch: 412 Loss: 0.05649933021093083\n",
            "Epoch: 413 Loss: 0.1128649919955414\n",
            "Epoch: 414 Loss: 0.15325330882067417\n",
            "Epoch: 415 Loss: 0.06902827588350785\n",
            "Epoch: 416 Loss: 0.09031531252135057\n",
            "Epoch: 417 Loss: 0.1277017597709018\n",
            "Epoch: 418 Loss: 0.14727407083233296\n",
            "Epoch: 419 Loss: 0.10957694159700088\n",
            "Epoch: 420 Loss: 0.12819502101396407\n",
            "Epoch: 421 Loss: 0.10246849152871182\n",
            "Epoch: 422 Loss: 0.24736916406428955\n",
            "Epoch: 423 Loss: 0.15107328289802902\n",
            "Epoch: 424 Loss: 0.144415112213525\n",
            "Epoch: 425 Loss: 0.12349899551640442\n",
            "Epoch: 426 Loss: 0.08671232217456032\n",
            "Epoch: 427 Loss: 0.07624529359436383\n",
            "Epoch: 428 Loss: 0.08252795495424507\n",
            "Epoch: 429 Loss: 0.06241280018813601\n",
            "Epoch: 430 Loss: 0.11159822341698213\n",
            "Epoch: 431 Loss: 0.12138999623557835\n",
            "Epoch: 432 Loss: 0.07579991496277959\n",
            "Epoch: 433 Loss: 0.07871969321237524\n",
            "Epoch: 434 Loss: 0.11555934072264877\n",
            "Epoch: 435 Loss: 0.06312961423735076\n",
            "Epoch: 436 Loss: 0.11707064309386091\n",
            "Epoch: 437 Loss: 0.08211109041801846\n",
            "Epoch: 438 Loss: 0.12370687117425283\n",
            "Epoch: 439 Loss: 0.09009394399653162\n",
            "Epoch: 440 Loss: 0.1553602623679548\n",
            "Epoch: 441 Loss: 0.10073845166458638\n",
            "Epoch: 442 Loss: 0.10447843722514585\n",
            "Epoch: 443 Loss: 0.13273521606540378\n",
            "Epoch: 444 Loss: 0.11886169949446646\n",
            "Epoch: 445 Loss: 0.12687138181404362\n",
            "Epoch: 446 Loss: 0.13249878487669775\n",
            "Epoch: 447 Loss: 0.15535947324589297\n",
            "Epoch: 448 Loss: 0.11496796428045075\n",
            "Epoch: 449 Loss: 0.16630089952190158\n",
            "Epoch: 450 Loss: 0.13869903943053574\n",
            "Epoch: 451 Loss: 0.11415637072543477\n",
            "Epoch: 452 Loss: 0.12142520121319707\n",
            "Epoch: 453 Loss: 0.06249542229818655\n",
            "Epoch: 454 Loss: 0.12874004136254757\n",
            "Epoch: 455 Loss: 0.1315648750112481\n",
            "Epoch: 456 Loss: 0.12532470129393508\n",
            "Epoch: 457 Loss: 0.1322304198940224\n",
            "Epoch: 458 Loss: 0.09843993991392053\n",
            "Epoch: 459 Loss: 0.07456282129337435\n",
            "Epoch: 460 Loss: 0.12265324749720523\n",
            "Epoch: 461 Loss: 0.1491830869871383\n",
            "Epoch: 462 Loss: 0.16914357794872326\n",
            "Epoch: 463 Loss: 0.08044200411148726\n",
            "Epoch: 464 Loss: 0.14304751729988285\n",
            "Epoch: 465 Loss: 0.13787855588909625\n",
            "Epoch: 466 Loss: 0.18021863840086777\n",
            "Epoch: 467 Loss: 0.10623799893465856\n",
            "Epoch: 468 Loss: 0.06544712384264724\n",
            "Epoch: 469 Loss: 0.08967339587367174\n",
            "Epoch: 470 Loss: 0.08136538725911192\n",
            "Epoch: 471 Loss: 0.11714852406108657\n",
            "Epoch: 472 Loss: 0.04303353904481768\n",
            "Epoch: 473 Loss: 0.07832354326917706\n",
            "Epoch: 474 Loss: 0.06065212260917599\n",
            "Epoch: 475 Loss: 0.11091925230655955\n",
            "Epoch: 476 Loss: 0.14601301711915962\n",
            "Epoch: 477 Loss: 0.19809050222669042\n",
            "Epoch: 478 Loss: 0.09633585629030401\n",
            "Epoch: 479 Loss: 0.10107535221593295\n",
            "Epoch: 480 Loss: 0.07936777489864405\n",
            "Epoch: 481 Loss: 0.05219891822697019\n",
            "Epoch: 482 Loss: 0.10654572998838287\n",
            "Epoch: 483 Loss: 0.10852482413460099\n",
            "Epoch: 484 Loss: 0.10114950581263076\n",
            "Epoch: 485 Loss: 0.12556221971945308\n",
            "Epoch: 486 Loss: 0.0674067207606159\n",
            "Epoch: 487 Loss: 0.15446108855457805\n",
            "Epoch: 488 Loss: 0.09984407108027692\n",
            "Epoch: 489 Loss: 0.1133864920210613\n",
            "Epoch: 490 Loss: 0.142905801934675\n",
            "Epoch: 491 Loss: 0.038705537471155\n",
            "Epoch: 492 Loss: 0.11690594516166469\n",
            "Epoch: 493 Loss: 0.11848452101900285\n",
            "Epoch: 494 Loss: 0.10886862918267726\n",
            "Epoch: 495 Loss: 0.061869010966838445\n",
            "Epoch: 496 Loss: 0.10948408349812735\n",
            "Epoch: 497 Loss: 0.03792741019099054\n",
            "Epoch: 498 Loss: 0.13102406142255174\n",
            "Epoch: 499 Loss: 0.0408971359906818\n",
            "Epoch: 500 Loss: 0.06433637538314974\n",
            "Epoch: 501 Loss: 0.12305904425079164\n",
            "Epoch: 502 Loss: 0.10805575788575114\n",
            "Epoch: 503 Loss: 0.11924868547378083\n",
            "Epoch: 504 Loss: 0.14034128191783118\n",
            "Epoch: 505 Loss: 0.158117212618115\n",
            "Epoch: 506 Loss: 0.07237276137739995\n",
            "Epoch: 507 Loss: 0.05253148002680868\n",
            "Epoch: 508 Loss: 0.12009939434551853\n",
            "Epoch: 509 Loss: 0.07953904173921766\n",
            "Epoch: 510 Loss: 0.09832019117343638\n",
            "Epoch: 511 Loss: 0.11602630503618039\n",
            "Epoch: 512 Loss: 0.08160192471792241\n",
            "Epoch: 513 Loss: 0.15394077434373343\n",
            "Epoch: 514 Loss: 0.06042977970610852\n",
            "Epoch: 515 Loss: 0.09239774539869733\n",
            "Epoch: 516 Loss: 0.12267032272973234\n",
            "Epoch: 517 Loss: 0.14973730436976293\n",
            "Epoch: 518 Loss: 0.13631510533153016\n",
            "Epoch: 519 Loss: 0.1492157103823068\n",
            "Epoch: 520 Loss: 0.1693550242477117\n",
            "Epoch: 521 Loss: 0.09019044522934633\n",
            "Epoch: 522 Loss: 0.0965947989200217\n",
            "Epoch: 523 Loss: 0.18284571129368687\n",
            "Epoch: 524 Loss: 0.1967282265119302\n",
            "Epoch: 525 Loss: 0.06800589542660784\n",
            "Epoch: 526 Loss: 0.07277024472057708\n",
            "Epoch: 527 Loss: 0.12037352927808453\n",
            "Epoch: 528 Loss: 0.12457528010928519\n",
            "Epoch: 529 Loss: 0.15758167589282165\n",
            "Epoch: 530 Loss: 0.09228159127566343\n",
            "Epoch: 531 Loss: 0.09172428092529562\n",
            "Epoch: 532 Loss: 0.21832115089592266\n",
            "Epoch: 533 Loss: 0.09933058819682118\n",
            "Epoch: 534 Loss: 0.1092507564353986\n",
            "Epoch: 535 Loss: 0.10247004264447536\n",
            "Epoch: 536 Loss: 0.13281568744826483\n",
            "Epoch: 537 Loss: 0.09262100118513254\n",
            "Epoch: 538 Loss: 0.06008265433633017\n",
            "Epoch: 539 Loss: 0.15783715894532577\n",
            "Epoch: 540 Loss: 0.1939984474890297\n",
            "Epoch: 541 Loss: 0.09364468137687486\n",
            "Epoch: 542 Loss: 0.05250427382791642\n",
            "Epoch: 543 Loss: 0.13670979518703635\n",
            "Epoch: 544 Loss: 0.10946572560298085\n",
            "Epoch: 545 Loss: 0.10114539622703822\n",
            "Epoch: 546 Loss: 0.19337080660421196\n",
            "Epoch: 547 Loss: 0.13616536250218436\n",
            "Epoch: 548 Loss: 0.09136321346648915\n",
            "Epoch: 549 Loss: 0.10245350323530704\n",
            "Epoch: 550 Loss: 0.06335938051027817\n",
            "Epoch: 551 Loss: 0.13146601515510342\n",
            "Epoch: 552 Loss: 0.16873388046613652\n",
            "Epoch: 553 Loss: 0.11017617866394516\n",
            "Epoch: 554 Loss: 0.08237732628216757\n",
            "Epoch: 555 Loss: 0.14876932844072172\n",
            "Epoch: 556 Loss: 0.09919140868964188\n",
            "Epoch: 557 Loss: 0.15358445304598814\n",
            "Epoch: 558 Loss: 0.11019891575965801\n",
            "Epoch: 559 Loss: 0.11561737924741565\n",
            "Epoch: 560 Loss: 0.09954285732199131\n",
            "Epoch: 561 Loss: 0.13891233117380827\n",
            "Epoch: 562 Loss: 0.13650498521211396\n",
            "Epoch: 563 Loss: 0.1691538968728203\n",
            "Epoch: 564 Loss: 0.13833551938555666\n",
            "Epoch: 565 Loss: 0.08643385334828965\n",
            "Epoch: 566 Loss: 0.14380763756439757\n",
            "Epoch: 567 Loss: 0.13821713111612316\n",
            "Epoch: 568 Loss: 0.08157485885193191\n",
            "Epoch: 569 Loss: 0.1033196599618709\n",
            "Epoch: 570 Loss: 0.12128011913732431\n",
            "Epoch: 571 Loss: 0.17582187484730694\n",
            "Epoch: 572 Loss: 0.13421383668609393\n",
            "Epoch: 573 Loss: 0.11435705628375104\n",
            "Epoch: 574 Loss: 0.10545353857986714\n",
            "Epoch: 575 Loss: 0.1422518901649462\n",
            "Epoch: 576 Loss: 0.12090035156425205\n",
            "Epoch: 577 Loss: 0.10554027787492382\n",
            "Epoch: 578 Loss: 0.13768878645664495\n",
            "Epoch: 579 Loss: 0.16064027411025383\n",
            "Epoch: 580 Loss: 0.11629769797042283\n",
            "Epoch: 581 Loss: 0.148914142994787\n",
            "Epoch: 582 Loss: 0.08761694122805688\n",
            "Epoch: 583 Loss: 0.12783615336703053\n",
            "Epoch: 584 Loss: 0.10734483230504352\n",
            "Epoch: 585 Loss: 0.1398653996350968\n",
            "Epoch: 586 Loss: 0.10979448946162318\n",
            "Epoch: 587 Loss: 0.14727598749050286\n",
            "Epoch: 588 Loss: 0.08669965065934322\n",
            "Epoch: 589 Loss: 0.12196761495484251\n",
            "Epoch: 590 Loss: 0.035157239683275346\n",
            "Epoch: 591 Loss: 0.13076130423754748\n",
            "Epoch: 592 Loss: 0.10299746651677917\n",
            "Epoch: 593 Loss: 0.11907930224012891\n",
            "Epoch: 594 Loss: 0.10018493256904144\n",
            "Epoch: 595 Loss: 0.13434000719601819\n",
            "Epoch: 596 Loss: 0.20404970920046078\n",
            "Epoch: 597 Loss: 0.08936892674995706\n",
            "Epoch: 598 Loss: 0.10280614848334824\n",
            "Epoch: 599 Loss: 0.11904279332779544\n",
            "Epoch: 600 Loss: 0.1323385398929272\n",
            "Epoch: 601 Loss: 0.1398564123018553\n",
            "Epoch: 602 Loss: 0.1230196853715599\n",
            "Epoch: 603 Loss: 0.14913470488698366\n",
            "Epoch: 604 Loss: 0.16439090123104064\n",
            "Epoch: 605 Loss: 0.18786885563748718\n",
            "Epoch: 606 Loss: 0.09382905204229906\n",
            "Epoch: 607 Loss: 0.07931172102769277\n",
            "Epoch: 608 Loss: 0.12342947442370905\n",
            "Epoch: 609 Loss: 0.11076768874226768\n",
            "Epoch: 610 Loss: 0.15237706496878717\n",
            "Epoch: 611 Loss: 0.1533351651113723\n",
            "Epoch: 612 Loss: 0.14321594094883033\n",
            "Epoch: 613 Loss: 0.17473044557216597\n",
            "Epoch: 614 Loss: 0.1388403798215069\n",
            "Epoch: 615 Loss: 0.09408980928525723\n",
            "Epoch: 616 Loss: 0.15206460356956114\n",
            "Epoch: 617 Loss: 0.08514677040717558\n",
            "Epoch: 618 Loss: 0.12239528936964372\n",
            "Epoch: 619 Loss: 0.12209842028238535\n",
            "Epoch: 620 Loss: 0.07439496229411746\n",
            "Epoch: 621 Loss: 0.07924246521765065\n",
            "Epoch: 622 Loss: 0.119651452477026\n",
            "Epoch: 623 Loss: 0.11020178061016864\n",
            "Epoch: 624 Loss: 0.07732188119565703\n",
            "Epoch: 625 Loss: 0.15148925969599752\n",
            "Epoch: 626 Loss: 0.02806591272834243\n",
            "Epoch: 627 Loss: 0.0678582244681118\n",
            "Epoch: 628 Loss: 0.14742938817405105\n",
            "Epoch: 629 Loss: 0.13468802515804104\n",
            "Epoch: 630 Loss: 0.12325764848542001\n",
            "Epoch: 631 Loss: 0.10670402263064324\n",
            "Epoch: 632 Loss: 0.09284380731244998\n",
            "Epoch: 633 Loss: 0.06529053358405837\n",
            "Epoch: 634 Loss: 0.13799361759145656\n",
            "Epoch: 635 Loss: 0.14388580362042724\n",
            "Epoch: 636 Loss: 0.11352166986863671\n",
            "Epoch: 637 Loss: 0.17507772743888922\n",
            "Epoch: 638 Loss: 0.07966193340835115\n",
            "Epoch: 639 Loss: 0.10316727937877523\n",
            "Epoch: 640 Loss: 0.06841452555508173\n",
            "Epoch: 641 Loss: 0.10564879369170906\n",
            "Epoch: 642 Loss: 0.08800229540098874\n",
            "Epoch: 643 Loss: 0.10541505536201776\n",
            "Epoch: 644 Loss: 0.14036598241063436\n",
            "Epoch: 645 Loss: 0.14279517297968453\n",
            "Epoch: 646 Loss: 0.13087639427693715\n",
            "Epoch: 647 Loss: 0.04361538169342237\n",
            "Epoch: 648 Loss: 0.09590384149094835\n",
            "Epoch: 649 Loss: 0.12954904726542554\n",
            "Epoch: 650 Loss: 0.1123634287408656\n",
            "Epoch: 651 Loss: 0.08332966085282782\n",
            "Epoch: 652 Loss: 0.08933819145783606\n",
            "Epoch: 653 Loss: 0.05462309909113048\n",
            "Epoch: 654 Loss: 0.14204429215350578\n",
            "Epoch: 655 Loss: 0.0584777239126446\n",
            "Epoch: 656 Loss: 0.09585943884777585\n",
            "Epoch: 657 Loss: 0.16568045062884226\n",
            "Epoch: 658 Loss: 0.08280337405527091\n",
            "Epoch: 659 Loss: 0.11230655635731365\n",
            "Epoch: 660 Loss: 0.17145864298239444\n",
            "Epoch: 661 Loss: 0.11130170730856123\n",
            "Epoch: 662 Loss: 0.18421144642236514\n",
            "Epoch: 663 Loss: 0.1503884707388665\n",
            "Epoch: 664 Loss: 0.056322829005093944\n",
            "Epoch: 665 Loss: 0.0783473367038317\n",
            "Epoch: 666 Loss: 0.09253153471184532\n",
            "Epoch: 667 Loss: 0.08303841151005177\n",
            "Epoch: 668 Loss: 0.06596345793670465\n",
            "Epoch: 669 Loss: 0.1474040672638087\n",
            "Epoch: 670 Loss: 0.18603470569724834\n",
            "Epoch: 671 Loss: 0.1390303705842027\n",
            "Epoch: 672 Loss: 0.15944102677770752\n",
            "Epoch: 673 Loss: 0.08094796750472535\n",
            "Epoch: 674 Loss: 0.12980219023050715\n",
            "Epoch: 675 Loss: 0.06318389225451376\n",
            "Epoch: 676 Loss: 0.13272185216838522\n",
            "Epoch: 677 Loss: 0.09988846520420856\n",
            "Epoch: 678 Loss: 0.1629978597581738\n",
            "Epoch: 679 Loss: 0.12152765528768661\n",
            "Epoch: 680 Loss: 0.15133152897863814\n",
            "Epoch: 681 Loss: 0.181236766854262\n",
            "Epoch: 682 Loss: 0.1033640941669877\n",
            "Epoch: 683 Loss: 0.08303436647378491\n",
            "Epoch: 684 Loss: 0.12057461736333291\n",
            "Epoch: 685 Loss: 0.14353513396773115\n",
            "Epoch: 686 Loss: 0.10505485132303485\n",
            "Epoch: 687 Loss: 0.11841802089517761\n",
            "Epoch: 688 Loss: 0.0796913796056877\n",
            "Epoch: 689 Loss: 0.1045685869428164\n",
            "Epoch: 690 Loss: 0.11953044086524747\n",
            "Epoch: 691 Loss: 0.11157326313029783\n",
            "Epoch: 692 Loss: 0.056903978195987234\n",
            "Epoch: 693 Loss: 0.12455856226547943\n",
            "Epoch: 694 Loss: 0.104179786029903\n",
            "Epoch: 695 Loss: 0.08983515024021867\n",
            "Epoch: 696 Loss: 0.09551947840114072\n",
            "Epoch: 697 Loss: 0.1393808009293261\n",
            "Epoch: 698 Loss: 0.1202134330782106\n",
            "Epoch: 699 Loss: 0.10555829831325414\n",
            "Epoch: 700 Loss: 0.09323958414484401\n",
            "Epoch: 701 Loss: 0.11955077610327805\n",
            "Epoch: 702 Loss: 0.052610317449868646\n",
            "Epoch: 703 Loss: 0.0770935619277495\n",
            "Epoch: 704 Loss: 0.13001158019257522\n",
            "Epoch: 705 Loss: 0.11260843153314695\n",
            "Epoch: 706 Loss: 0.12928924842254577\n",
            "Epoch: 707 Loss: 0.10727054235970052\n",
            "Epoch: 708 Loss: 0.11290798459211052\n",
            "Epoch: 709 Loss: 0.06636698536371188\n",
            "Epoch: 710 Loss: 0.09500014516472993\n",
            "Epoch: 711 Loss: 0.0474969226392185\n",
            "Epoch: 712 Loss: 0.07796024329714751\n",
            "Epoch: 713 Loss: 0.06121253205793719\n",
            "Epoch: 714 Loss: 0.1307373221445489\n",
            "Epoch: 715 Loss: 0.1726674883234775\n",
            "Epoch: 716 Loss: 0.0774355525391955\n",
            "Epoch: 717 Loss: 0.08575593444985877\n",
            "Epoch: 718 Loss: 0.1338985206526583\n",
            "Epoch: 719 Loss: 0.09719260967089353\n",
            "Epoch: 720 Loss: 0.16783191468256287\n",
            "Epoch: 721 Loss: 0.06013577727095249\n",
            "Epoch: 722 Loss: 0.11924573086435065\n",
            "Epoch: 723 Loss: 0.09743163078448341\n",
            "Epoch: 724 Loss: 0.07453125719916805\n",
            "Epoch: 725 Loss: 0.14800330587080074\n",
            "Epoch: 726 Loss: 0.06297257739364516\n",
            "Epoch: 727 Loss: 0.1659065661317938\n",
            "Epoch: 728 Loss: 0.052667928348783054\n",
            "Epoch: 729 Loss: 0.0639920408803425\n",
            "Epoch: 730 Loss: 0.061634231144774176\n",
            "Epoch: 731 Loss: 0.05073616229165057\n",
            "Epoch: 732 Loss: 0.10785524826020362\n",
            "Epoch: 733 Loss: 0.12552098230004027\n",
            "Epoch: 734 Loss: 0.16818212919768488\n",
            "Epoch: 735 Loss: 0.13659471610186846\n",
            "Epoch: 736 Loss: 0.10115155703080571\n",
            "Epoch: 737 Loss: 0.067153421387483\n",
            "Epoch: 738 Loss: 0.09889055350001948\n",
            "Epoch: 739 Loss: 0.049086169705800396\n",
            "Epoch: 740 Loss: 0.1313421247214775\n",
            "Epoch: 741 Loss: 0.043942010268568574\n",
            "Epoch: 742 Loss: 0.06831196781969551\n",
            "Epoch: 743 Loss: 0.14795710499055026\n",
            "Epoch: 744 Loss: 0.11785453276608886\n",
            "Epoch: 745 Loss: 0.09915754504387231\n",
            "Epoch: 746 Loss: 0.09558459207046852\n",
            "Epoch: 747 Loss: 0.10816617315083851\n",
            "Epoch: 748 Loss: 0.06564887493508115\n",
            "Epoch: 749 Loss: 0.09519619956186212\n",
            "Epoch: 750 Loss: 0.05580867372722374\n",
            "Epoch: 751 Loss: 0.09977672408246852\n",
            "Epoch: 752 Loss: 0.08145445350363975\n",
            "Epoch: 753 Loss: 0.08402161820306896\n",
            "Epoch: 754 Loss: 0.09854262115648167\n",
            "Epoch: 755 Loss: 0.13416866302680747\n",
            "Epoch: 756 Loss: 0.13791079375104404\n",
            "Epoch: 757 Loss: 0.1306824714092063\n",
            "Epoch: 758 Loss: 0.10613092187643494\n",
            "Epoch: 759 Loss: 0.10148970763964785\n",
            "Epoch: 760 Loss: 0.13888528959705634\n",
            "Epoch: 761 Loss: 0.10993092579945071\n",
            "Epoch: 762 Loss: 0.033985012201547704\n",
            "Epoch: 763 Loss: 0.13992636866126623\n",
            "Epoch: 764 Loss: 0.04677669240367028\n",
            "Epoch: 765 Loss: 0.11918669493049594\n",
            "Epoch: 766 Loss: 0.06709390643139268\n",
            "Epoch: 767 Loss: 0.11972537919323578\n",
            "Epoch: 768 Loss: 0.10604459950826683\n",
            "Epoch: 769 Loss: 0.12375367048486886\n",
            "Epoch: 770 Loss: 0.06129423448539656\n",
            "Epoch: 771 Loss: 0.07781250172919985\n",
            "Epoch: 772 Loss: 0.09090639428531247\n",
            "Epoch: 773 Loss: 0.18788144651506528\n",
            "Epoch: 774 Loss: 0.12298381392532387\n",
            "Epoch: 775 Loss: 0.07703080810386276\n",
            "Epoch: 776 Loss: 0.15176748954476244\n",
            "Epoch: 777 Loss: 0.086147922840329\n",
            "Epoch: 778 Loss: 0.09760730684624117\n",
            "Epoch: 779 Loss: 0.09959020144114533\n",
            "Epoch: 780 Loss: 0.149334659986446\n",
            "Epoch: 781 Loss: 0.05748851317155882\n",
            "Epoch: 782 Loss: 0.10810781583454891\n",
            "Epoch: 783 Loss: 0.10343053792815894\n",
            "Epoch: 784 Loss: 0.049344894013550515\n",
            "Epoch: 785 Loss: 0.047165783885924475\n",
            "Epoch: 786 Loss: 0.07716038738932149\n",
            "Epoch: 787 Loss: 0.15885071982783028\n",
            "Epoch: 788 Loss: 0.10913707974091306\n",
            "Epoch: 789 Loss: 0.11247479014272455\n",
            "Epoch: 790 Loss: 0.12844212829071444\n",
            "Epoch: 791 Loss: 0.20459333608640828\n",
            "Epoch: 792 Loss: 0.062031191904651325\n",
            "Epoch: 793 Loss: 0.16825344611992332\n",
            "Epoch: 794 Loss: 0.11176858304457761\n",
            "Epoch: 795 Loss: 0.06709779279082373\n",
            "Epoch: 796 Loss: 0.10949530816336105\n",
            "Epoch: 797 Loss: 0.10900729715865826\n",
            "Epoch: 798 Loss: 0.11957523471944184\n",
            "Epoch: 799 Loss: 0.06899076123474604\n",
            "Epoch: 800 Loss: 0.11792323327165827\n",
            "Epoch: 801 Loss: 0.13470088147658077\n",
            "Epoch: 802 Loss: 0.13552246127491302\n",
            "Epoch: 803 Loss: 0.040533435423274686\n",
            "Epoch: 804 Loss: 0.055707763100121814\n",
            "Epoch: 805 Loss: 0.08856540845099081\n",
            "Epoch: 806 Loss: 0.14664040510232723\n",
            "Epoch: 807 Loss: 0.07549166919963224\n",
            "Epoch: 808 Loss: 0.029524745145002654\n",
            "Epoch: 809 Loss: 0.05725224066018326\n",
            "Epoch: 810 Loss: 0.09012992502368421\n",
            "Epoch: 811 Loss: 0.09164993773062556\n",
            "Epoch: 812 Loss: 0.1330711958736983\n",
            "Epoch: 813 Loss: 0.14904983515185638\n",
            "Epoch: 814 Loss: 0.08588209170932419\n",
            "Epoch: 815 Loss: 0.11105282233641334\n",
            "Epoch: 816 Loss: 0.12006885133632728\n",
            "Epoch: 817 Loss: 0.0776844624329987\n",
            "Epoch: 818 Loss: 0.1264919717111128\n",
            "Epoch: 819 Loss: 0.07643406666175663\n",
            "Epoch: 820 Loss: 0.09474342753382801\n",
            "Epoch: 821 Loss: 0.11618790217849502\n",
            "Epoch: 822 Loss: 0.13564809364347724\n",
            "Epoch: 823 Loss: 0.11026731890590324\n",
            "Epoch: 824 Loss: 0.061918858643391264\n",
            "Epoch: 825 Loss: 0.17006424382385532\n",
            "Epoch: 826 Loss: 0.07761294474496291\n",
            "Epoch: 827 Loss: 0.10874567001993664\n",
            "Epoch: 828 Loss: 0.05861879614891441\n",
            "Epoch: 829 Loss: 0.1261321373580169\n",
            "Epoch: 830 Loss: 0.07530660466996605\n",
            "Epoch: 831 Loss: 0.062017020038831736\n",
            "Epoch: 832 Loss: 0.03290423683059179\n",
            "Epoch: 833 Loss: 0.06787757267057991\n",
            "Epoch: 834 Loss: 0.07131160778605042\n",
            "Epoch: 835 Loss: 0.07246434183255601\n",
            "Epoch: 836 Loss: 0.13140054459305067\n",
            "Epoch: 837 Loss: 0.06027197673397392\n",
            "Epoch: 838 Loss: 0.06342829195926437\n",
            "Epoch: 839 Loss: 0.11792795162741233\n",
            "Epoch: 840 Loss: 0.11814102915210228\n",
            "Epoch: 841 Loss: 0.07900909456407143\n",
            "Epoch: 842 Loss: 0.060687926811178634\n",
            "Epoch: 843 Loss: 0.1428148190924948\n",
            "Epoch: 844 Loss: 0.08985468991715428\n",
            "Epoch: 845 Loss: 0.12221109846696648\n",
            "Epoch: 846 Loss: 0.12504478317967543\n",
            "Epoch: 847 Loss: 0.11441958744310923\n",
            "Epoch: 848 Loss: 0.05647901474850625\n",
            "Epoch: 849 Loss: 0.13037154347935287\n",
            "Epoch: 850 Loss: 0.06577782866198152\n",
            "Epoch: 851 Loss: 0.12396125825025228\n",
            "Epoch: 852 Loss: 0.11885437825680173\n",
            "Epoch: 853 Loss: 0.16656901105549318\n",
            "Epoch: 854 Loss: 0.09460048963901904\n",
            "Epoch: 855 Loss: 0.15539804661968554\n",
            "Epoch: 856 Loss: 0.0496631498561376\n",
            "Epoch: 857 Loss: 0.06473884049615343\n",
            "Epoch: 858 Loss: 0.10513970905235803\n",
            "Epoch: 859 Loss: 0.08216357168194836\n",
            "Epoch: 860 Loss: 0.13760827445244722\n",
            "Epoch: 861 Loss: 0.0822403788017732\n",
            "Epoch: 862 Loss: 0.1291032123551622\n",
            "Epoch: 863 Loss: 0.07764953545063336\n",
            "Epoch: 864 Loss: 0.06732969659044599\n",
            "Epoch: 865 Loss: 0.17546716155162645\n",
            "Epoch: 866 Loss: 0.12201882965866777\n",
            "Epoch: 867 Loss: 0.07393783385734193\n",
            "Epoch: 868 Loss: 0.0737896939842794\n",
            "Epoch: 869 Loss: 0.14876367647052063\n",
            "Epoch: 870 Loss: 0.08108746772495996\n",
            "Epoch: 871 Loss: 0.12096819419044685\n",
            "Epoch: 872 Loss: 0.10210590397076266\n",
            "Epoch: 873 Loss: 0.056615715047453746\n",
            "Epoch: 874 Loss: 0.024558781855548016\n",
            "Epoch: 875 Loss: 0.05307508900187496\n",
            "Epoch: 876 Loss: 0.14801432079380014\n",
            "Epoch: 877 Loss: 0.12881979630424525\n",
            "Epoch: 878 Loss: 0.08594845470327098\n",
            "Epoch: 879 Loss: 0.08791743953793045\n",
            "Epoch: 880 Loss: 0.0753405810081069\n",
            "Epoch: 881 Loss: 0.045577759815812705\n",
            "Epoch: 882 Loss: 0.08489853876804941\n",
            "Epoch: 883 Loss: 0.08408049620801339\n",
            "Epoch: 884 Loss: 0.074135418506245\n",
            "Epoch: 885 Loss: 0.06769194291475036\n",
            "Epoch: 886 Loss: 0.05888810180600662\n",
            "Epoch: 887 Loss: 0.18351832116131805\n",
            "Epoch: 888 Loss: 0.05754343927101825\n",
            "Epoch: 889 Loss: 0.16955952907106314\n",
            "Epoch: 890 Loss: 0.041622449456127496\n",
            "Epoch: 891 Loss: 0.141772073491118\n",
            "Epoch: 892 Loss: 0.0721970372312965\n",
            "Epoch: 893 Loss: 0.08715055114332057\n",
            "Epoch: 894 Loss: 0.11521940166725728\n",
            "Epoch: 895 Loss: 0.13131097802240796\n",
            "Epoch: 896 Loss: 0.06604452656529411\n",
            "Epoch: 897 Loss: 0.17093144117056996\n",
            "Epoch: 898 Loss: 0.13731323750229102\n",
            "Epoch: 899 Loss: 0.16963780049560112\n",
            "Epoch: 900 Loss: 0.12315561928466881\n",
            "Epoch: 901 Loss: 0.06549220124049532\n",
            "Epoch: 902 Loss: 0.06270971074399083\n",
            "Epoch: 903 Loss: 0.146345699372828\n",
            "Epoch: 904 Loss: 0.14383530226114455\n",
            "Epoch: 905 Loss: 0.04288998830267666\n",
            "Epoch: 906 Loss: 0.12227765482430429\n",
            "Epoch: 907 Loss: 0.09399358698013244\n",
            "Epoch: 908 Loss: 0.08190555741828331\n",
            "Epoch: 909 Loss: 0.05959444138460477\n",
            "Epoch: 910 Loss: 0.05909940371081954\n",
            "Epoch: 911 Loss: 0.08241672970736993\n",
            "Epoch: 912 Loss: 0.03917972107875784\n",
            "Epoch: 913 Loss: 0.12002029174193744\n",
            "Epoch: 914 Loss: 0.08339610473483976\n",
            "Epoch: 915 Loss: 0.07283772359857144\n",
            "Epoch: 916 Loss: 0.1089702691249827\n",
            "Epoch: 917 Loss: 0.13381074448143815\n",
            "Epoch: 918 Loss: 0.0535403847432762\n",
            "Epoch: 919 Loss: 0.06492753655554101\n",
            "Epoch: 920 Loss: 0.12162086188099258\n",
            "Epoch: 921 Loss: 0.15859140718805362\n",
            "Epoch: 922 Loss: 0.06148899308527195\n",
            "Epoch: 923 Loss: 0.09465852051003909\n",
            "Epoch: 924 Loss: 0.10350498069322406\n",
            "Epoch: 925 Loss: 0.08206851806860511\n",
            "Epoch: 926 Loss: 0.10160206771156469\n",
            "Epoch: 927 Loss: 0.13003140017840414\n",
            "Epoch: 928 Loss: 0.1039800015609259\n",
            "Epoch: 929 Loss: 0.08385109219129025\n",
            "Epoch: 930 Loss: 0.07030254802796607\n",
            "Epoch: 931 Loss: 0.1302564490975153\n",
            "Epoch: 932 Loss: 0.18812589085023723\n",
            "Epoch: 933 Loss: 0.04512603157800503\n",
            "Epoch: 934 Loss: 0.13968796402292583\n",
            "Epoch: 935 Loss: 0.112382108445376\n",
            "Epoch: 936 Loss: 0.12426307389261215\n",
            "Epoch: 937 Loss: 0.124046437350509\n",
            "Epoch: 938 Loss: 0.08382538916194117\n",
            "Epoch: 939 Loss: 0.12653188012452962\n",
            "Epoch: 940 Loss: 0.07897191777886371\n",
            "Epoch: 941 Loss: 0.07375433733733264\n",
            "Epoch: 942 Loss: 0.20546480764383274\n",
            "Epoch: 943 Loss: 0.049524898832688014\n",
            "Epoch: 944 Loss: 0.08647632809022837\n",
            "Epoch: 945 Loss: 0.11863396264096032\n",
            "Epoch: 946 Loss: 0.05052540481884098\n",
            "Epoch: 947 Loss: 0.048684952326689526\n",
            "Epoch: 948 Loss: 0.205718764892777\n",
            "Epoch: 949 Loss: 0.1114077567332582\n",
            "Epoch: 950 Loss: 0.07755612003377617\n",
            "Epoch: 951 Loss: 0.10681034685948744\n",
            "Epoch: 952 Loss: 0.11425690968586982\n",
            "Epoch: 953 Loss: 0.11570638442069212\n",
            "Epoch: 954 Loss: 0.034618948658661365\n",
            "Epoch: 955 Loss: 0.09300463681476497\n",
            "Epoch: 956 Loss: 0.11875769158149825\n",
            "Epoch: 957 Loss: 0.044306206358179445\n",
            "Epoch: 958 Loss: 0.10320144553187016\n",
            "Epoch: 959 Loss: 0.12232499587384864\n",
            "Epoch: 960 Loss: 0.12078717837706623\n",
            "Epoch: 961 Loss: 0.08359206560004895\n",
            "Epoch: 962 Loss: 0.04761698441955556\n",
            "Epoch: 963 Loss: 0.08396698817370463\n",
            "Epoch: 964 Loss: 0.07907420120076307\n",
            "Epoch: 965 Loss: 0.0457693490031641\n",
            "Epoch: 966 Loss: 0.04480680465536153\n",
            "Epoch: 967 Loss: 0.11538334594788367\n",
            "Epoch: 968 Loss: 0.12593673639328784\n",
            "Epoch: 969 Loss: 0.11210066575763727\n",
            "Epoch: 970 Loss: 0.10594620993505707\n",
            "Epoch: 971 Loss: 0.08941895647407196\n",
            "Epoch: 972 Loss: 0.11230444508432508\n",
            "Epoch: 973 Loss: 0.07054156091927516\n",
            "Epoch: 974 Loss: 0.09222753009129564\n",
            "Epoch: 975 Loss: 0.0739566494309314\n",
            "Epoch: 976 Loss: 0.06934216284699875\n",
            "Epoch: 977 Loss: 0.07988898485242293\n",
            "Epoch: 978 Loss: 0.13775632679168875\n",
            "Epoch: 979 Loss: 0.11779180119780501\n",
            "Epoch: 980 Loss: 0.07127564904476785\n",
            "Epoch: 981 Loss: 0.11730755104372528\n",
            "Epoch: 982 Loss: 0.08826872805546546\n",
            "Epoch: 983 Loss: 0.10072349704849323\n",
            "Epoch: 984 Loss: 0.058815898272980074\n",
            "Epoch: 985 Loss: 0.07380231291494899\n",
            "Epoch: 986 Loss: 0.07249477336581167\n",
            "Epoch: 987 Loss: 0.12234878043181416\n",
            "Epoch: 988 Loss: 0.08656571736612617\n",
            "Epoch: 989 Loss: 0.12248341526578949\n",
            "Epoch: 990 Loss: 0.0646910055348621\n",
            "Epoch: 991 Loss: 0.09943941271391515\n",
            "Epoch: 992 Loss: 0.05639596058557124\n",
            "Epoch: 993 Loss: 0.10504059729649351\n",
            "Epoch: 994 Loss: 0.14680612891383557\n",
            "Epoch: 995 Loss: 0.150963536623601\n",
            "Epoch: 996 Loss: 0.15263561521278554\n",
            "Epoch: 997 Loss: 0.09195106930213505\n",
            "Epoch: 998 Loss: 0.047703014590540146\n",
            "Epoch: 999 Loss: 0.08660694019073609\n",
            "0.8708588573571696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_image(X):\n",
        "    plt.imshow(X.reshape(28, 28), cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "data_point = np.random.random_integers(1, 7999)\n",
        "X = data_X[data_point]\n",
        "plot_image(X)\n",
        "\n",
        "Y = data_Y[data_point]\n",
        "Y_hat = neuralnetwork.predict(X)\n",
        "\n",
        "print(\"Predicted: \" + str(np.argmax(Y_hat)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "J5T6lobla7Nc",
        "outputId": "3ee9a084-d3f5-4e5f-d5ba-c021a25e4689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: 2\n"
          ]
        }
      ]
    }
  ]
}